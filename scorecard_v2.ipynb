{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scorecardpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application = pd.read_csv('data/IS453 Group Assignment - Application Data.csv')\n",
    "bureau = pd.read_csv('data/IS453 Group Assignment - Bureau Data.csv')\n",
    "\n",
    "print(application.shape)\n",
    "print(bureau.shape)\n",
    "\n",
    "# filter for applicants that do not own a car\n",
    "application = application[application['FLAG_OWN_CAR'] == 'N']\n",
    "\n",
    "# filter bureau dataset for CREDIT_CURRENCY = currency 1\n",
    "bureau = bureau[bureau['CREDIT_CURRENCY'] == 'currency 1']\n",
    "\n",
    "# drop some useless and potentially problematic columns\n",
    "application = application.drop(columns=['FLAG_OWN_CAR', 'OWN_CAR_AGE', 'CODE_GENDER'])\n",
    "bureau = bureau.drop(columns=['CREDIT_CURRENCY'])\n",
    "\n",
    "print(application.shape)\n",
    "print(bureau.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and drop invalid outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum reasonable days_credit_update = 80 years * 365 days = 29,200 days\n",
    "max_days_credit_update = -(80*365)\n",
    "\n",
    "# find rows that exceed that threshold\n",
    "bureau[bureau['DAYS_CREDIT_UPDATE'] < max_days_credit_update]\n",
    "\n",
    "# sample code\n",
    "\n",
    "# check number of rows before, drop those rows, and then check after\n",
    "print(bureau.shape[0])\n",
    "bureau.drop(bureau[bureau['DAYS_CREDIT_UPDATE'] < max_days_credit_update].index, inplace = True)\n",
    "print(bureau.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bureau['DAYS_CREDIT_UPDATE'].max())\n",
    "print(bureau['DAYS_CREDIT_UPDATE'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop negative values in AMT_CREDIT_SUM_LIMIT\n",
    "initial = bureau.shape[0]\n",
    "bureau.drop(bureau[bureau['AMT_CREDIT_SUM_LIMIT'] < 0].index, inplace=True)\n",
    "final = bureau.shape[0]\n",
    "\n",
    "print(initial)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bureau['AMT_CREDIT_SUM_LIMIT'].min())\n",
    "# print(bureau['DAYS_CREDIT_UPDATE'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values in Bureau data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series((bureau.isnull().sum().sort_values(ascending=False)/bureau.shape[0])).map(\"{0:.0%}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with high percentage of missing values\n",
    "bureau = bureau.drop(columns=['AMT_ANNUITY', 'AMT_CREDIT_MAX_OVERDUE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with high percentage of missing values\n",
    "bureau = bureau.drop(columns=['DAYS_ENDDATE_FACT', 'AMT_CREDIT_SUM_LIMIT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt_credit_sum_debt = bureau['AMT_CREDIT_SUM_DEBT']\n",
    "amt_credit_sum_debt.fillna(value = 0, inplace = True)\n",
    "# bureau['AMT_CREDIT_SUM_DEBT'] = amt_credit_sum_debt\n",
    "\n",
    "print(amt_credit_sum_debt.value_counts())\n",
    "print(\"n/a       \", amt_credit_sum_debt.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_credit_enddate = bureau['DAYS_CREDIT_ENDDATE']\n",
    "days_credit_enddate.fillna(value = 0, inplace = True)\n",
    "\n",
    "print(days_credit_enddate.value_counts())\n",
    "print(\"n/a       \", days_credit_enddate.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series((bureau.isnull().sum().sort_values(ascending=False)/bureau.shape[0])).map(\"{0:.0%}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values in Application data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Print the percentage of missing values for each column\n",
    "print(pd.Series((application.isnull().sum().sort_values(ascending=False) / application.shape[0])).map(\"{0:.0%}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentages = application.isnull().sum() / application.shape[0]\n",
    "\n",
    "# Drop columns with more than 65% missing values\n",
    "columns_to_drop = missing_percentages[missing_percentages > 0.34].index\n",
    "application.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Print the percentage of missing values for remaining columns (optional)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(pd.Series((application.isnull().sum().sort_values(ascending=False) / application.shape[0])).map(\"{0:.0%}\".format))\n",
    "print(application.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_combine = [\n",
    "    'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
    "    'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "    'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "    'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "    'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "    'AMT_REQ_CREDIT_BUREAU_HOUR'\n",
    "]\n",
    "\n",
    "# Create a new column that is the sum of these columns\n",
    "application['AMT_REQ_CREDIT_BUREAU_TOTAL'] = application[columns_to_combine].sum(axis=1)\n",
    "application.drop(columns=columns_to_combine, inplace=True)\n",
    "application.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series((application.isnull().sum().sort_values(ascending=False) / application.shape[0])).map(\"{0:.0%}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series((bureau.isnull().sum().sort_values(ascending=False) / bureau.shape[0])).map(\"{0:.0%}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Preliminary Screening by IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_bureau = bureau.copy()\n",
    "\n",
    "# precompute credit status counts\n",
    "agg_bureau['NUM_ACTIVE_CREDITS'] = (agg_bureau['CREDIT_ACTIVE'] == 'Active').astype(int)\n",
    "agg_bureau['NUM_CLOSED_CREDITS'] = (agg_bureau['CREDIT_ACTIVE'] == 'Closed').astype(int)\n",
    "agg_bureau['NUM_BADDEBT_CREDITS'] = (agg_bureau['CREDIT_ACTIVE'] == 'Bad debt').astype(int)\n",
    "\n",
    "# precompute top 5 most common credit type counts\n",
    "agg_bureau['NUM_CONSUMER_CREDIT_LOANS'] = (agg_bureau['CREDIT_TYPE'] == 'Consumer credit').astype(int)\n",
    "agg_bureau['NUM_CREDIT_CARD_LOANS'] = (agg_bureau['CREDIT_TYPE'] == 'Credit card').astype(int)\n",
    "agg_bureau['NUM_CAR_LOANS'] = (agg_bureau['CREDIT_TYPE'] == 'Car loan').astype(int)\n",
    "agg_bureau['NUM_MORTGAGE_LOANS'] = (agg_bureau['CREDIT_TYPE'] == 'Mortgage').astype(int)\n",
    "agg_bureau['NUM_MICRO_LOANS'] = (agg_bureau['CREDIT_TYPE'] == 'Microloan').astype(int)\n",
    "\n",
    "# DEBT_CREDIT_RATIO = AMT_CREDIT_SUM_DEBT / AMT_CREDIT_SUM\n",
    "agg_bureau['DEBT_CREDIT_RATIO'] = agg_bureau['AMT_CREDIT_SUM_DEBT'] / agg_bureau['AMT_CREDIT_SUM']\n",
    "\n",
    "# flatten bureau data\n",
    "agg_bureau = agg_bureau.groupby('SK_ID_CURR').agg(\n",
    "    NUM_PREV_LOANS = ('SK_ID_BUREAU', 'count'),\n",
    "    NUM_ACTIVE_CREDITS = ('NUM_ACTIVE_CREDITS', 'sum'),\n",
    "    NUM_CLOSED_CREDITS = ('NUM_CLOSED_CREDITS', 'sum'),\n",
    "    NUM_CONSUMER_CREDIT_LOANS = ('NUM_CONSUMER_CREDIT_LOANS', 'sum'),\n",
    "    NUM_CREDIT_CARD_LOANS = ('NUM_CREDIT_CARD_LOANS', 'sum'),\n",
    "    NUM_CAR_LOANS = ('NUM_CAR_LOANS', 'sum'),\n",
    "    NUM_MORTGAGE_LOANS = ('NUM_MORTGAGE_LOANS', 'sum'),\n",
    "    NUM_MICRO_LOANS = ('NUM_MICRO_LOANS', 'sum'),\n",
    "    DAYS_CREDIT_MIN = ('DAYS_CREDIT', 'min'), # capture the oldest credit application\n",
    "    DAYS_CREDIT_MAX = ('DAYS_CREDIT', 'max'), # find most recent credit application\n",
    "    DAYS_CREDIT_MEAN = ('DAYS_CREDIT', 'mean'), # avg days before current application, for prev credit lines\n",
    "    DAYS_CREDIT_OVERDUE_MAX = ('CREDIT_DAY_OVERDUE', 'max'),\n",
    "    DAYS_CREDIT_OVERDUE_MEAN = ('CREDIT_DAY_OVERDUE', 'mean'),\n",
    "    DAYS_CREDIT_ENDDATE_MEAN = ('DAYS_CREDIT_ENDDATE', 'mean'),\n",
    "    CNT_CREDIT_PROLONG_MAX = ('CNT_CREDIT_PROLONG', 'max'),\n",
    "    CNT_CREDIT_PROLONG_MEAN = ('CNT_CREDIT_PROLONG', 'mean'),\n",
    "    # AMT_CREDIT_SUM_LIMIT_MEAN = ('AMT_CREDIT_SUM_LIMIT', 'mean'),\n",
    "    AMT_CREDIT_SUM_OVERDUE_SUM = ('AMT_CREDIT_SUM_OVERDUE', 'sum'),\n",
    "    AMT_CREDIT_SUM_OVERDUE_MAX = ('AMT_CREDIT_SUM_OVERDUE', 'max'),\n",
    "    DAYS_CREDIT_UPDATE_MEAN = ('DAYS_CREDIT_UPDATE', 'mean'),\n",
    "    DEBT_CREDIT_RATIO_MEAN = ('DEBT_CREDIT_RATIO', 'mean'),\n",
    "    # AMT_ANNUITY_MEAN = ('AMT_ANNUITY', 'mean'),\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(application.shape)\n",
    "print(bureau.shape)\n",
    "print(agg_bureau.shape)\n",
    "\n",
    "merged = pd.merge(left=application, right=agg_bureau, on='SK_ID_CURR', how='left')\n",
    "print(merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_working = merged.copy()\n",
    "cols = merged_working.columns[1:]\n",
    "merged_working = merged_working.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unable to calculate WOE & IV for ORGANIZATION_TYPE as it is because it has too many possible categories\n",
    "    # group some values together\n",
    "\n",
    "mapping = {\n",
    "    'Business Entity Type 1': 'Business Entity',\n",
    "    'Business Entity Type 2': 'Business Entity',\n",
    "    'Business Entity Type 3': 'Business Entity',\n",
    "    'Trade: type 7': 'Trade',\n",
    "    'Trade: type 3': 'Trade',\n",
    "    'Trade: type 2': 'Trade',\n",
    "    'Trade: type 6': 'Trade',\n",
    "    'Trade: type 1': 'Trade',\n",
    "    'Trade: type 4': 'Trade',\n",
    "    'Trade: type 5': 'Trade',\n",
    "    'Transport: type 1': 'Transport',\n",
    "    'Transport: type 2': 'Transport',\n",
    "    'Transport: type 3': 'Transport',\n",
    "    'Transport: type 4': 'Transport',\n",
    "    'Industry: type 3': 'Industry',\n",
    "    'Industry: type 11': 'Industry',\n",
    "    'Industry: type 9': 'Industry',\n",
    "    'Industry: type 7': 'Industry',\n",
    "    'Industry: type 1': 'Industry',\n",
    "    'Industry: type 4': 'Industry',\n",
    "    'Industry: type 5': 'Industry',\n",
    "    'Industry: type 6': 'Industry',\n",
    "    'Industry: type 2': 'Industry',\n",
    "    'Industry: type 10': 'Industry',\n",
    "    'Industry: type 12': 'Industry',\n",
    "    'Industry: type 13': 'Industry',\n",
    "    'Industry: type 8': 'Industry',\n",
    "    'Government': 'Public Sector',\n",
    "    'Transport': 'Public Sector',\n",
    "    'Military': 'Public Sector',\n",
    "    'Police': 'Public Sector',\n",
    "    'Hotel': 'Hospitality',\n",
    "    'Restaurant': 'Hospitality',\n",
    "    'Bank': 'Financial Services',\n",
    "    'Insurance': 'Financial Services',\n",
    "}\n",
    "\n",
    "merged_working['ORGANIZATION_TYPE'] = merged_working['ORGANIZATION_TYPE'].replace(mapping)\n",
    "print(merged_working['ORGANIZATION_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_shape = iv_above_threshold.shape\n",
    "# print(og_shape)\n",
    "\n",
    "# Calculate the percentage of rows with more than 50 missing values\n",
    "pct_total_rows_missing = merged_working[merged_working.isnull().sum(axis = 1) > 50].shape[0] / merged_working.shape[0]\n",
    "print(f'Percent of total rows missing more 50 than values: {pct_total_rows_missing:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_working.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with more than 50 missing values per row\n",
    "# for_analysis = merged_working.dropna(thresh = merged_working.shape[1] - 50)\n",
    "\n",
    "# print(f'Ending row count: {for_analysis.shape[0]}')\n",
    "# print(f'Percent dropped: {(1 - for_analysis.shape[0] / og_shape[0]):.2%}')\n",
    "\n",
    "rows_org = merged_working.shape[0]\n",
    "print(f'Starting row count: {rows_org}')\n",
    "\n",
    "# drop rows based on a threshold of more than 35 missing values per row\n",
    "merged_working = merged_working.dropna(thresh = merged_working.shape[1] - 50)\n",
    "print(f'Ending row count: {merged_working.shape[0]}')\n",
    "print(f'Percent dropped: {(1 - merged_working.shape[0] / rows_org):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(merged_working.isnull().sum().sort_values(ascending=False)/merged_working.shape[0]).map(\"{0:.0%}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each col\n",
    "missing_data = merged_working.isnull().mean().sort_values(ascending=False)\n",
    "missing_data_over_50 = missing_data[missing_data > 0.5].apply(lambda x: \"{:.0%}\".format(x))\n",
    "missing_data_over_50\n",
    "merged_working_copy = merged_working.copy()\n",
    "\n",
    "merged_working = merged_working.drop(columns=missing_data_over_50.index)\n",
    "merged_working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = merged_working.select_dtypes(include='number')\n",
    "max = numeric_data.max()\n",
    "min = numeric_data.min()\n",
    "print(\"Max \\n\", max)\n",
    "print(\"Min \\n\", min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bin ranges, WOE, and IV for the independent variables\n",
    "bins = sc.woebin(merged_working, y='TARGET')\n",
    "\n",
    "# create a dictionary to store the IVs for each variable\n",
    "iv_dict = {}\n",
    "\n",
    "# reverse WOE and store IV values in the dictionary\n",
    "for variable, bindetails in bins.items():\n",
    "    bins[variable]['woe'] = bins[variable]['woe'] * -1\n",
    "    iv_value = bindetails['total_iv'][0]\n",
    "    iv_dict[variable] = iv_value\n",
    "    display(bindetails)\n",
    "\n",
    "# Sort the dictionary by IV values in descending order\n",
    "iv_dict_sorted = dict(sorted(iv_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sorted IV values\n",
    "for variable, iv in iv_dict_sorted.items():\n",
    "    print(f\"{variable}: IV = {iv:.4f}\")\n",
    "print(len(iv_dict_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of columns with IV less than 0.02\n",
    "columns_below_threshold = [variable for variable, iv in iv_dict_sorted.items() if iv < 0.02]\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of columns with IV below 0.02: {len(columns_below_threshold)}\")\n",
    "\n",
    "# Drop columns with IV less than 0.02\n",
    "iv_above_threshold = merged_working.drop(columns=columns_below_threshold)\n",
    "\n",
    "print(f\"Number of columns left after dropping: {len(iv_above_threshold.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Rows with too many Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with more than 35 missing values per row\n",
    "for_analysis = iv_above_threshold.dropna(thresh = iv_above_threshold.shape[1] - 35)\n",
    "\n",
    "print(f'Ending row count: {for_analysis.shape[0]}')\n",
    "print(f'Percent dropped: {(1 - for_analysis.shape[0] / og_shape[0]):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(merged_working.isnull().sum().sort_values(ascending=False)/merged_working.shape[0]).map(\"{0:.0%}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each col\n",
    "missing_data = merged_working.isnull().mean().sort_values(ascending=False)\n",
    "missing_data_over_50 = missing_data[missing_data > 0.5].apply(lambda x: \"{:.0%}\".format(x))\n",
    "missing_data_over_50\n",
    "merged_working_copy = merged_working.copy()\n",
    "\n",
    "merged_working = merged_working.drop(columns=missing_data_over_50)\n",
    "merged_working.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni & Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(for_analysis.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_corr_matrix(data, threshold):\n",
    "\n",
    "    numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "    # Compute the correlation matrix for the numerical columns\n",
    "    correlation_matrix = data[numeric_columns].corr().round(3)\n",
    "\n",
    "    # find values in matrix greater than threshold\n",
    "    temp = correlation_matrix[abs(correlation_matrix) > threshold]\n",
    "\n",
    "    # Plot the correlation matrix using a heatmap for better visualization\n",
    "    fig, ax = plt.subplots(figsize=(18,8))\n",
    "    sns.heatmap(temp, xticklabels=temp.columns, yticklabels=temp.columns, annot=True, ax=ax)\n",
    "    ax.grid(color='gray', linestyle='--', linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Dropping Columns\n",
    "- numerical columns: dropped by correlation, business logic, and IV\n",
    "- categorical columns: check value counts and IV, drop if col is potentially problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning = for_analysis.copy()\n",
    "\n",
    "# drop cols related to household (based on business logic, these are irrelevant)\n",
    "to_drop = [\n",
    "    'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_AVG', \n",
    "    'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAREA_AVG',\n",
    "    'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE','ELEVATORS_MODE', \n",
    "    'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAREA_MODE', \n",
    "    'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'ELEVATORS_MEDI',\n",
    "    'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAREA_MEDI', \n",
    "    'TOTALAREA_MODE', 'WALLSMATERIAL_MODE'\n",
    "]\n",
    "cleaning = for_analysis.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable to capture the percentage of closed credits\n",
    "cleaning['PCT_CLOSED_CREDITS'] = (merged_working['NUM_CLOSED_CREDITS'] / (merged_working['NUM_CLOSED_CREDITS'] + merged_working['NUM_ACTIVE_CREDITS']))\n",
    "\n",
    "# drop the 2 original columns\n",
    "to_drop = [\n",
    "    'NUM_CLOSED_CREDITS', 'NUM_ACTIVE_CREDITS'\n",
    "]\n",
    "cleaning = cleaning.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find high correlations\n",
    "print_corr_matrix(cleaning, 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amt credit, amt annuity, amt goods price have high correlation\n",
    "    # let's check the amount of null values they have\n",
    "temp = cleaning[['AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE']]\n",
    "print(temp.isna().sum())\n",
    "\n",
    "# let's also check their IVs\n",
    "print(round(iv_dict['AMT_CREDIT'],4))\n",
    "print(round(iv_dict['AMT_ANNUITY'],4))\n",
    "print(round(iv_dict['AMT_GOODS_PRICE'],4))\n",
    "\n",
    "# based on all the info, let's drop rows where AMT_GOODS_PRICE is empty\n",
    "    # then combine AMT_GOODS_PRICE and AMT_CREDIT as a new variable\n",
    "    # then drop the original 3 columns\n",
    "cleaning = cleaning.dropna(subset=['AMT_GOODS_PRICE'])\n",
    "cleaning['PCT_AMT_CREDIT_TO_GOODSPRICE'] = cleaning['AMT_CREDIT'] / cleaning['AMT_GOODS_PRICE']\n",
    "to_drop = ['AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE']\n",
    "cleaning = cleaning.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of nulls & IV of variables with high correlations\n",
    "temp = [\n",
    "    'FLAG_EMP_PHONE', 'DAYS_EMPLOYED', \n",
    "    'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', \n",
    "    'LIVE_CITY_NOT_WORK_CITY', 'REG_CITY_NOT_WORK_CITY', \n",
    "    'DAYS_CREDIT_MIN', 'DAYS_CREDIT_MEAN', 'DAYS_CREDIT_MAX', 'DAYS_CREDIT_UPDATE_MEAN'\n",
    "]\n",
    "\n",
    "print(cleaning[temp].isna().sum())\n",
    "for col in temp:\n",
    "    print(f\"{col}: {round(iv_dict[col],4)}\")\n",
    "\n",
    "# based on the printed info, we can drop the following columns\n",
    "to_drop = ['FLAG_EMP_PHONE', 'REGION_RATING_CLIENT', 'LIVE_CITY_NOT_WORK_CITY', 'DAYS_CREDIT_MIN', 'DAYS_CREDIT_MAX', 'DAYS_CREDIT_UPDATE_MEAN']\n",
    "cleaning = cleaning.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based some potentially iffy / irrelevant categorical columns\n",
    "    # also checked the IVs for these columns\n",
    "to_drop = [\n",
    "    'NAME_EDUCATION_TYPE', # may be biased, IV = 0.0403\n",
    "    'NAME_FAMILY_STATUS', # may be biased, IV = 0.0227\n",
    "    'HOUSETYPE_MODE', # may be biased, IV = 0.0232\n",
    "    'EMERGENCYSTATE_MODE' # housing related, may be irrelevant, IV = 0.0252\n",
    "]\n",
    "cleaning = cleaning.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaning.columns.tolist())\n",
    "print(len(cleaning.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Categorical Columns\n",
    "- bin them using scorecard py \n",
    "- get the bins and map the values\n",
    "    - to one hot encode after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaning.select_dtypes(include='object').columns.tolist())\n",
    "print(len(cleaning.select_dtypes(include='object').columns))\n",
    "print(len(cleaning.select_dtypes(include=['float64', 'int64']).columns))\n",
    "\n",
    "temp = cleaning.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = sc.woebin(temp[['TARGET', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE']], y='TARGET')\n",
    "\n",
    "for variable, bindetails in bins.items():\n",
    "    bins[variable]['woe'] = bins[variable]['woe'] * -1\n",
    "    # print('IV: ' + str(round(bindetails['total_iv'][0], 4)))\n",
    "    # display(bindetails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate WOE and IV manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_iv(data, variable_name):\n",
    "    working_data = data.copy()\n",
    "\n",
    "    # Handle missing values\n",
    "    working_data[variable_name].fillna('Missing', inplace=True)\n",
    "\n",
    "    # Create a new dataframe for the calculations\n",
    "    df = pd.DataFrame()\n",
    "    df['Bins'] = working_data[variable_name].unique()\n",
    "    df['Count'] = df['Bins'].map(working_data[variable_name].value_counts())\n",
    "    df['Events'] = df['Bins'].map(working_data.groupby(variable_name)['TARGET'].sum())\n",
    "    df['Non_Events'] = df['Count'] - df['Events']\n",
    "    df['%_Events'] = df['Events'] / sum(df['Events'])\n",
    "    df['%_Non_Events'] = df['Non_Events'] / sum(df['Non_Events'])\n",
    "    df['WOE'] = np.log(df['%_Non_Events'] / df['%_Events'])\n",
    "    df['IV'] = (df['%_Non_Events'] - df['%_Events']) * df['WOE']\n",
    "\n",
    "    IV = sum(df['IV'])\n",
    "\n",
    "    return IV, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle NAME_INCOME_TYPE variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IV: ' + str(round(bins['NAME_INCOME_TYPE']['total_iv'][0], 4)))\n",
    "display(bins['NAME_INCOME_TYPE'])\n",
    "\n",
    "# print the bins\n",
    "print(bins['NAME_INCOME_TYPE']['bin'].tolist())\n",
    "\n",
    "iv, df = woe_iv(temp, 'NAME_INCOME_TYPE')\n",
    "df.sort_values(by='IV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the breaks bc original breaks don't make sense\n",
    "breaks = {'NAME_INCOME_TYPE': [\n",
    "    'Working%,%Maternity leave%,%Businessman%,%Student', 'State servant%,%Unemployed%,%Commercial associate', 'Pensioner'\n",
    "]}\n",
    "bins2 = sc.woebin(temp[['TARGET', 'NAME_INCOME_TYPE']], y='TARGET', breaks_list=breaks)\n",
    "for variable, bindetails in bins2.items():\n",
    "    bins2[variable]['woe'] = bins2[variable]['woe'] * -1\n",
    "    display(bindetails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the bins into temp dataframe\n",
    "mapping = {\n",
    "    'Businessman': 'Working_Businessman_Maternity leave_Student',\n",
    "    'Maternity leave': 'Working_Businessman_Maternity leave_Student',\n",
    "    'Student': 'Working_Businessman_Maternity leave_Student',\n",
    "    'Working': 'Working_Businessman_Maternity leave_Student',\n",
    "    'Commercial associate': 'State servant_Unemployed_Commercial associate',\n",
    "    'State servant': 'State servant_Unemployed_Commercial associate',\n",
    "    'Unemployed': 'State servant_Unemployed_Commercial associate',\n",
    "}\n",
    "# save IVs of the individual bins for later\n",
    "iv_dict_categorical = {\n",
    "    'NAME_INCOME_TYPE_Working_Businessman_Maternity leave_Student': 0.024634,\n",
    "    'NAME_INCOME_TYPE_State servant_Unemployed_Commercial associate': 0.003169,\n",
    "    'NAME_INCOME_TYPE_Pensioner': 0.039381,\n",
    "}\n",
    "temp['NAME_INCOME_TYPE'] = temp['NAME_INCOME_TYPE'].replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle OCCUPATION_TYPE variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IV: ' + str(round(bins['OCCUPATION_TYPE']['total_iv'][0], 4)))\n",
    "display(bins['OCCUPATION_TYPE'])\n",
    "\n",
    "# print the bins\n",
    "print(bins['OCCUPATION_TYPE']['bin'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['OCCUPATION_TYPE'].fillna('missing', inplace=True)\n",
    "mapping = {\n",
    "    'Accountants': 'bin1',\n",
    "    'Core staff': 'bin1',\n",
    "    'Managers': 'bin1',\n",
    "    'IT staff': 'bin1',\n",
    "    'High skill tech staff': 'bin1',\n",
    "    'HR staff': 'bin1',\n",
    "    'Private service staff': 'bin1',\n",
    "    'Medicine staff': 'bin1',\n",
    "    'Secretaries': 'bin1',\n",
    "    'Realty agents': 'bin1',\n",
    "    'Cleaning staff': 'bin2',\n",
    "    'Sales staff': 'bin2',\n",
    "    'Cooking staff': 'bin2',\n",
    "    'Security staff': 'bin3',\n",
    "    'Laborers': 'bin3',\n",
    "    'Waiters/barmen staff': 'bin3',\n",
    "    'Drivers': 'bin3',\n",
    "    'Low-skill Laborers': 'bin3'\n",
    "}\n",
    "temp['OCCUPATION_TYPE'] = temp['OCCUPATION_TYPE'].replace(mapping)\n",
    "\n",
    "# save the IVs\n",
    "iv_dict_categorical['OCCUPATION_TYPE_bin1'] = 0.014435\n",
    "iv_dict_categorical['OCCUPATION_TYPE_bin2'] = 0.006753\n",
    "iv_dict_categorical['OCCUPATION_TYPE_bin3'] = 0.045982\n",
    "iv_dict_categorical['OCCUPATION_TYPE_missing'] = 0.023020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle ORGANIZATION_TYPE variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IV: ' + str(round(bins['ORGANIZATION_TYPE']['total_iv'][0], 4)))\n",
    "display(bins['ORGANIZATION_TYPE'])\n",
    "\n",
    "# print the bins\n",
    "print(bins['ORGANIZATION_TYPE']['bin'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'University': 'bin1',\n",
    "    'Culture': 'bin1',\n",
    "    'Security Ministries': 'bin1',\n",
    "    'XNA': 'bin1',\n",
    "    'Financial Services': 'bin1',\n",
    "    'School': 'bin1',\n",
    "    'Services': 'bin2',\n",
    "    'Public Sector': 'bin2',\n",
    "    'Medicine': 'bin2',\n",
    "    'Religion': 'bin2',\n",
    "    'Emergency': 'bin2',\n",
    "    'Kindergarten': 'bin2',\n",
    "    'Other': 'bin3',\n",
    "    'Electricity': 'bin3',\n",
    "    'Telecom': 'bin3',\n",
    "    'Housing': 'bin3',\n",
    "    'Postal': 'bin3',\n",
    "    'Cleaning': 'bin3',\n",
    "    'Trade': 'bin3',\n",
    "    'Industry': 'bin3',\n",
    "    'Legal Services': 'bin3',\n",
    "    'Business Entity': 'bin3',\n",
    "    'Mobile': 'bin3',\n",
    "    'Advertising': 'bin3',\n",
    "    'Transport': 'bin3',\n",
    "    'Self-employed': 'bin3',\n",
    "    'Hospitality': 'bin3',\n",
    "    'Agriculture': 'bin3',\n",
    "    'Security': 'bin3',\n",
    "    'Construction': 'bin3',\n",
    "    'Realtor': 'bin3',\n",
    "}\n",
    "temp['ORGANIZATION_TYPE'] = temp['ORGANIZATION_TYPE'].replace(mapping)\n",
    "\n",
    "# save the IVs\n",
    "iv_dict_categorical['ORGANIZATION_TYPE_bin1'] = 0.047089\n",
    "iv_dict_categorical['ORGANIZATION_TYPE_bin2'] = 0.003997\n",
    "iv_dict_categorical['ORGANIZATION_TYPE_bin3'] = 0.024165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy encoding and then calculate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = temp.select_dtypes(include='object').columns.tolist()\n",
    "encoded = pd.get_dummies(temp, columns=cats, drop_first=False, dtype=int)\n",
    "encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those with low IV, below 0.02\n",
    "print(iv_dict_categorical)\n",
    "to_drop = [key for key, value in iv_dict_categorical.items() if value < 0.02]\n",
    "encoded = encoded.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "print_corr_matrix(encoded, 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check those with high correlation\n",
    "print(iv_dict['DAYS_EMPLOYED'])\n",
    "print(iv_dict_categorical['NAME_INCOME_TYPE_Pensioner'])\n",
    "print(iv_dict_categorical['OCCUPATION_TYPE_missing'])\n",
    "print(iv_dict_categorical['ORGANIZATION_TYPE_bin1'])\n",
    "print(iv_dict_categorical['ORGANIZATION_TYPE_bin3'])\n",
    "\n",
    "# based on this info, let's drop these columns\n",
    "to_drop = ['NAME_INCOME_TYPE_Pensioner', 'OCCUPATION_TYPE_missing', 'ORGANIZATION_TYPE_bin1', 'ORGANIZATION_TYPE_bin3']\n",
    "encoded = encoded.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop more columns (yall can play around w what you drop here)\n",
    "- for scorecard tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = sc.woebin(encoded, y='TARGET')\n",
    "\n",
    "# initialise dict\n",
    "iv_dict = {}\n",
    "\n",
    "for variable, bindetails in bins.items():\n",
    "    bins[variable]['woe'] = bins[variable]['woe'] * -1\n",
    "    iv_dict[variable] = round(bindetails['total_iv'][0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_dict = dict(sorted(iv_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(f'Total number of rows: {len(encoded)}')\n",
    "\n",
    "for key, value in iv_dict.items():\n",
    "    print(f'{key}: IV = {value}')\n",
    "    print(f'\\tNumber of missing values: {encoded[key].isna().sum()}')\n",
    "    print(f'\\tPercentage of missing values: {round(encoded[key].isna().sum() / len(encoded),3)}')\n",
    "\n",
    "print(encoded.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_corr_matrix(encoded, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    'FLAG_DOCUMENT_12', 'NUM_CREDIT_CARD_LOANS', 'REG_CITY_NOT_LIVE_CITY', # iv below 0.04\n",
    "    'FLAG_DOCUMENT_3', 'REGION_POPULATION_RELATIVE', 'DAYS_REGISTRATION', 'REG_CITY_NOT_WORK_CITY', # iv below 0.04\n",
    "    'DAYS_BIRTH', # potentially biased + relatively high correlation\n",
    "    'EXT_SOURCE_1', # many missing values (more than half the dataset)\n",
    "]\n",
    "\n",
    "encoded = encoded.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin WOEs to achieve monotonicity\n",
    "- source for Binning function: https://github.com/jstephenj14/Monotonic-WOE-Binning-Algorithm/blob/master/monotonic_binning/monotonic_woe_binning.py\n",
    "    - derives the bins where monotonicity is achieved\n",
    "- implementation: https://lelesgaray.github.io/blog/scorecard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install monotonic-binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monotonic_binning.monotonic_woe_binning import Binning \n",
    "\n",
    "# split data into test and train\n",
    "train, test = sc.split_df(encoded, 'TARGET', ratio=0.7, seed=42).values()\n",
    "\n",
    "x_vars = train.drop(['TARGET'], axis=1).columns\n",
    "y_var = train['TARGET']\n",
    "\n",
    "def get_breaks_for_monotonicity(x_vars, y_var_name):\n",
    "  bin_object = Binning(y_var_name, n_threshold=50, y_threshold=10, p_threshold=0.35, sign=False)\n",
    "  breaks = {}\n",
    "  for var in x_vars:\n",
    "    bin_object.fit(train[[y_var_name, var]])\n",
    "    breaks[var] = (bin_object.bins[1:-1].tolist())\n",
    "    print(f'check: {var} fitting done')\n",
    "  return breaks\n",
    "\n",
    "new_breaks = get_breaks_for_monotonicity(x_vars, 'TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = sc.woebin(encoded, y='TARGET', breaks_list=new_breaks, positive='bad|0') # change positive to reverse WOE sign\n",
    "sc.woebin_plot(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode WOE values\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    train_woe = sc.woebin_ply(train, bins)\n",
    "    test_woe = sc.woebin_ply(test, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X, y parts of data for train and test\n",
    "y_train = train_woe.loc[:, 'TARGET']\n",
    "X_train = train_woe.loc[:, train_woe.columns != 'TARGET']\n",
    "y_test = test_woe.loc[:, 'TARGET']\n",
    "X_test = test_woe.loc[:, train_woe.columns != 'TARGET']\n",
    "\n",
    "# create and fit model\n",
    "lr = linear_model.LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# find coefficients\n",
    "coeff = pd.Series(np.concatenate([lr.intercept_, lr.coef_[0]]),index = np.concatenate([['intercept'], lr.feature_names_in_]))\n",
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    card = sc.scorecard(bins, lr, X_train.columns, points0=600, odds0=1/20, pdo=20, basepoints_eq0=True)\n",
    "\n",
    "pprint.pprint(card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics of the model\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print('PCC measures:')\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance roc\n",
    "train_pred = lr.predict_proba(X_train)[:, 1]\n",
    "test_pred = lr.predict_proba(X_test)[:, 1]\n",
    "train_perf = sc.perf_eva(y_train, train_pred, plot_type = ['roc'], title = 'train')\n",
    "test_perf = sc.perf_eva(y_test, test_pred, plot_type = ['roc'], title = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
